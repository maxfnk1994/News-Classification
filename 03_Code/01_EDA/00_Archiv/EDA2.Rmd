---
title: "EDA analysis - Classification of news headlines with impact on the probability of stock prices changes"
author: Max Franke
date: March 30, 2020
output:
    pdf_document:
    number_sections: true
---
Course: CIS-627 CAPSTONE

Term: Spring/T2

Last modified date: 03/27/2020

Data: news headlines and descriptions ~5k news articles pertaining to several publicly traded companies

# Approach of the analysis

This analysis aims to visualize the collected data in order to include subsequent results and relationships in the further process of the project. The following procedure is applied:

First, a data set is examined that evaluates the mood of headlines and their descriptions with regard to different stocks in a labeled form. 

Based on this, the most frequently used words from headline and description are linked to google trends. The background to this approach is that the assumption is made that these words are representative of a word family that has an influence on stocks, so that the trend of these words can be examined more closely.


# Data presenation

In this section the data in its raw version is presented. 

```{r echo = FALSE}
## Clean Workspace
rm(list = ls())
```

```{r echo = FALSE}
## Install packages
#install.packages("tidyverse")
#install.packages("knitr")
#install.packages("rmarkdown")
#install.packages("tinytex")
#install.packages("dplyr")
#install.packages("kableExtra")
```

```{r warning=FALSE, message=FALSE, echo = FALSE}
## Load libraries
library(tidyverse)
library(knitr)
library(rmarkdown)
#tinytex::install_tinytex()
library(tinytex)
library(dplyr)
library(kableExtra)
options(knitr.table.format = "latex")
```

```{r warning=FALSE, message=FALSE, echo = FALSE}
## Load data of stocks with headlines and description
setwd("/Users/MaxFranke/Desktop/05_Big Data Analytics/04_Classes/04 SP:Term2/CIS-627 CAPSTONE/News-Classification/02_Data/02_EDA")
stock_text <- read.csv("Stock_Text_Symbol_new.csv")
```

```{r warning=FALSE, message=FALSE, echo = FALSE}
## First look at the data
knitr::kable(head(stock_text), "latex", 
             booktabs = T, 
             caption = "Dataset stocks with headlines and description") %>%
  kable_styling(full_width = TRUE) %>%
  column_spec(c(1,2), width = "5cm") %>%
  column_spec(c(3,4), width = "2cm") %>%
  row_spec(c(1,3,5), background = "gray")
  
```

## Description of data

The dataset shows four columns. The first two columns are “text”-columns (title and description). The third column shows the stock symbol. The fourth column is the labeling of the first three columns. It is divided into three classes:


1 if the news is positive for the company and may encourage people to buy shares.

0 if the news is neural or not possible to identify it as positive or negative

-1 if the news is negative for the company, bad publicity, or would discourage people from owning shares.

# Analyze the distribution

## Graph: Total numbers of symbols

The first graph shows the total numbers of symbols so we get a feeling about the distribution.

```{r echo=FALSE}
p1 <- ggplot(data = stock_text, mapping = aes(x = forcats::fct_infreq(symbol))) +
  geom_bar(mapping = aes(y = ..count.., fill = symbol)) +
  guides(fill = FALSE) +
  scale_x_discrete(labels = c("AAPL" = "Apple", "AMD" = "AMD", "AMZN" = "Amazon", "FB" = 
              "Facebook", "GOOGL" = "Google", "MSFT" = "Microsoft", "TSLA" = "Tesla")) +
  labs(title = "Total Numbers of Symbols",
       x = "Stock Symbols",
       y = "Count",
       caption = "Source: news headlines of ~5k news articles")
p1
```

### Interpretation of the graph:

Apple, facebook and amazon shows the highest hits. The sum of these three stocks are 46.9% of the total dataset.

In the next step, the relationship between the stocks per sentiment will be analyzed.




## Graph: Number of stock symbols per sentiment
### Description of the column sentiment (values 1,0,-1)

1: if the news is positive for the company and may encourage people to buy shares.

0: if the news is neural or not possible to identify it as positive or negative

-1: if the news is negative for the company, bad publicity, or would discourage people from owning shares.

```{r echo=FALSE}
# First, change the class of column sentiment to change the labels in ggplot
graph2 <- stock_text
graph2$sentiment <- as.character(graph2$sentiment)

# Plot: Barplot to compare number of stock symbol for the 3 different sentiments
p2 <- ggplot(data = graph2, mapping = aes(x = sentiment, fill = symbol)) +
  geom_bar(mapping = aes(y = ..prop..,group = symbol), position = "dodge") +
  scale_x_discrete(labels = c("1" = "positive", "0" = "neutral", "-1" = "negative")) + 
  theme(legend.position = "bottom") +
  labs(fill = "Stock symbols",
       title = "Number of Symbols per Sentiment",
       x = "Sentiment of news article",
       y = "Frequency",
       caption = "Source: news headlines of ~5k news articles")
p2
```

### Interpretation of the graph:

The frequency of symbols per sentiment is shown above.
It can be seen that facebook has a high number of bad publicity. 

In general, the frequency in the neutral area is the strongest (50% of all cases in the dataset)
Apple and Microsoft show a high number with positive news.

\newpage

# Word clouds

## Process of generating word clouds for the column titles

### Step 1: Extract and then load the titles from dataset

First, the column "titles" are extracted and saved to a txt.file. Then, this txt. file is load into R.

```{r echo=FALSE}
# Write column title to txt. file
write.table(stock_text$title, "title.txt", row.names = FALSE, col.names = FALSE)
text <- readLines("title.txt")

# Head of titles
kable(data.frame(Number = c(1:6), Titles = head(text)), "latex",
      longtable = T, booktabs = T,
      caption = "First 10 titles") %>%
  kable_styling(latex_options = c("repeat_header"))
```

### Step 2: Required packages

For this analysis, following packages are required:

 - tm (text mining)
 - snowball (text stemming)
 - wordcloud (generator for the visualization)
 - RColorBrewer (palettes of colors)

```{r echo=FALSE}

#install.packages("tm")  # for text mining
#install.packages("SnowballC") # for text stemming
#install.packages("wordcloud") # word-cloud generator 
#install.packages("RColorBrewer") # color palettes
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
```

### Step 3: Text mining

1. Load the txt. file as a corpus

```{r echo=FALSE}
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
```

2. Inspect the corpus

```{r echo=FALSE}
inspect(docs[1:10])
```


3. Text transformation
  - Replace special characters from text

```{r warning=FALSE, message=FALSE, echo=FALSE}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
```

4. Cleaning the text
 - Convert the text to lower case
 - Remove punctuations
 - Eliminate extra white spaces
 - Remove english common stopwords
 - Remove the name of the stocks

```{r warning=FALSE, message=FALSE, echo=FALSE}
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove the name of the stocks
docs <- tm_map(docs, removeWords, 
               c("apple", "amazon", "facebook", "google", "tesla", "microsoft", "facebooks", "apples"))
```

### Step 4: Building a term-document matrix
A term-document matris is a table which contains the frequency of the words.

```{r echo=FALSE}
dtm <- TermDocumentMatrix(docs)
dtm_matrix <- as.matrix(dtm)
dtm_vector <- sort(rowSums(dtm_matrix),decreasing=TRUE)
dtm_dataframe <- data.frame(word = names(dtm_vector),freq=dtm_vector)

kable(data.frame(head(dtm_dataframe, 10), row.names = NULL), "latex",
      longtable = T, booktabs = T,
      caption = "Top 10 words in title by frequency") %>%
  kable_styling(latex_options = c("repeat_header"))

```

### Step 5: Generate the word cloud

```{r echo=FALSE}
set.seed(1234)
layout(matrix(c(1, 2), nrow = 2), heights = c(1, 10))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Wordcloud of news headlines (company names excluded)")
wordcloud(words = dtm_dataframe$word, freq = dtm_dataframe$freq, min.freq = 20,
          max.words = 200, random.order = FALSE, rot.per = 0.2, 
          colors = brewer.pal(8, "Dark2"), 
          main = "Title")

```

### Interpretation of the graph:

Eyecatcher of this graph is the word "update", which will be analyzed in the following chapters.
Also, says and news are highlighted. 

Furthermore, words like model, tech and data are highlighted which is interesting and is maybe an indicator for the impact of the field "data science" in the stock market.


### Step 6: Frequent terms in the term-document matrix
```{r echo=FALSE}
kable(data.frame("Words 1-10" = findFreqTerms(dtm, lowfreq = 30,)[1:10],
           "Words 11-20" = findFreqTerms(dtm, lowfreq = 30,)[11:20],
           "Words 21-30" = findFreqTerms(dtm, lowfreq = 30,)[21:30],
           "Words 31-40" = findFreqTerms(dtm, lowfreq = 30,)[31:40],
           "Words 41-50" = findFreqTerms(dtm, lowfreq = 30,)[41:50],
           "Words 51-60" = findFreqTerms(dtm, lowfreq = 30,)[51:60],
           check.names = FALSE), "latex",
      longtable = T, booktabs = T,
      caption = "Frequent words in title") %>%
  kable_styling(latex_options = c("repeat_header"))
```

#### Identify the frequency of top 10 words
```{r echo=FALSE}
top_10 <- head(dtm_dataframe, 10)

kable(data.frame(top_10, row.names = NULL), "latex",
      longtable = T, booktabs = T,
      caption = "Frequency of top 10 words in title") %>%
  kable_styling(latex_options = c("repeat_header"))

```

### Graph: Word frequency for the top 10 words in the title
```{r echo=FALSE}
p4 <- ggplot(data = top_10, mapping = aes(x = reorder(word, freq), y = freq, fill = word)) +
  geom_col() +
  guides(fill = FALSE) +
  labs(title = "Frequency of the top 10 words in the title",
       subtitle = "company names excluded",
       x = "Top 10 words",
       y = "Frequency",
       caption = "Source: news headlines of ~5k news articles")
p4
```

### Interpretation of the graph:

The word "update" is by a wide range the most frequent word in the title.

Consequently, a further analysis concerning update and sentiment would be interesting.

### Graph: The word "update" in titles, split by sentiment
```{r echo=FALSE}
# Define dataframe
p3 <- stock_text
# Add update column
p3$update <- NA

# define values for possible options of update
update <- c("update", "updates", "Update", "UPDATE", "Updates", "UPDATES")

# check every row for values in title
for (i in 1:nrow(p3)) {
  for (m in 1:length(update)) {
    if (str_detect(p3[i,1], update[m]) == TRUE) {
    p3[i,5] <- "update"
    }
  }
}

# filter the target values
p3_update <- p3 %>%
                filter(update == "update")

# Change the class of column sentiment to change the labels in ggplot
graph3 <- p3_update
graph3$sentiment <- as.character(graph3$sentiment)

# Plot
p3_final <- ggplot(data = graph3, mapping = aes(x = sentiment)) +
        geom_bar(mapping = aes(y = ..count..), position = "dodge") +
        geom_bar(mapping = aes(y = ..count.., fill = symbol), position = "dodge", alpha = 0.6) +
        scale_x_discrete(labels = c("1" = "positive", "0" = "neutral", "-1" = "negative")) + 
        theme(legend.position = "bottom") +
        labs(fill = "Stock symbols",
             title = "Word update in titles divided by sentiment",
             x = "Sentiment of news article",
             y = "Frequency",
             caption = "Source: news headlines of ~5k news articles")
p3_final

```

### Interpretation of the graph:

The word "update" shows concerning the different values of sentiment kind of a normal distribution, which means that the most titles inlcuding the word "update" have a neutral sentiment. But there is a small trend that with the word update there is a negative sentiment in the news title.

Splitted by stocks, it is shown that facebook shows a high negative frequency, which was already shown in the Graph: Number of stock symbols per sentiment.



### Extract and then Load the description from dataset
```{r}
# Write column title to txt. file
write.table(stock_text$description, "description.txt", row.names = FALSE, 
            col.names = FALSE)
text <- readLines("description.txt")

# Head of titles
kable(data.frame(Number = c(1:6),Description = head(text)), "latex",
      longtable = T, booktabs = T,
      caption = "First 10 descriptions") %>%
  kable_styling(latex_options = c("repeat_header"), full_width = TRUE) %>%
  column_spec(column = 1, width = "1cm")
```

### Load the data as a corpus
```{r}
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
```

### Inspect the content of the document
```{r eval=FALSE}
inspect(docs[1:10])
```

### Text transformation
```{r warning=FALSE, message=FALSE}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
```

### Cleaning the text
```{r warning=FALSE, message=FALSE}
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove the name of the stocks
docs <- tm_map(docs, removeWords, 
               c("apple", "amazon", "facebook", "google", "tesla", "microsoft"))
```

### Building a term-document matrix
```{r}
dtm <- TermDocumentMatrix(docs)
dtm_matrix <- as.matrix(dtm)
dtm_vector <- sort(rowSums(dtm_matrix),decreasing=TRUE)
dtm_dataframe <- data.frame(word = names(dtm_vector),freq=dtm_vector)

kable(data.frame(head(dtm_dataframe, 10), row.names = NULL), "latex",
      longtable = T, booktabs = T,
      caption = "Top 10 words in description by frequency") %>%
  kable_styling(latex_options = c("repeat_header"))
```

### Generate the Word cloud
```{r warning=FALSE, message=FALSE}
set.seed(1234)
layout(matrix(c(1, 2), nrow = 2), heights = c(1, 10))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Wordcloud of news description (company names excluded)")
wordcloud(words = dtm_dataframe$word, freq = dtm_dataframe$freq, min.freq = 20,
          max.words = 200, random.order = FALSE, rot.per = 0.2, 
          colors = brewer.pal(8, "Dark2"), 
          main = "Title")

```

### Frequent terms in the term-document matrix
```{r}
kable(data.frame("Words 1-10" = findFreqTerms(dtm, lowfreq = 30,)[1:10],
           "Words 11-20" = findFreqTerms(dtm, lowfreq = 30,)[11:20],
           "Words 21-30" = findFreqTerms(dtm, lowfreq = 30,)[21:30],
           "Words 31-40" = findFreqTerms(dtm, lowfreq = 30,)[31:40],
           "Words 41-50" = findFreqTerms(dtm, lowfreq = 30,)[41:50],
           "Words 51-60" = findFreqTerms(dtm, lowfreq = 30,)[51:60],
           check.names = FALSE), "latex",
      longtable = T, booktabs = T,
      caption = "Frequent words in description") %>%
  kable_styling(latex_options = c("repeat_header"))
```

### Frequency of top 10 words
```{r}
top_10 <- head(dtm_dataframe, 10)

kable(data.frame(top_10, row.names = NULL), "latex",
      longtable = T, booktabs = T,
      caption = "Frequency of top 10 words in title") %>%
  kable_styling(latex_options = c("repeat_header"))
```

## Plot 6: Word frequency for the top 10
```{r}
p6 <- ggplot(data = top_10, mapping = aes(x = reorder(word, freq), y = freq, fill = word)) +
  geom_col() +
  guides(fill = FALSE) +
  labs(title = "Frequency of the top 10 words in the description",
       subtitle = "company names excluded",
       x = "Top 10 words",
       y = "Frequency",
       caption = "Source: news headlines of ~5k news articles")
p6
```



















































